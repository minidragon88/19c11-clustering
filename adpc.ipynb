{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adpc.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSj6a1hiTWPC",
        "colab_type": "text"
      },
      "source": [
        "Declaration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAMCm9Y7TaVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import urllib.request\n",
        "from sklearn import mixture\n",
        "from scipy.spatial import distance\n",
        "import math\n",
        "import statistics \n",
        "\n",
        "CUT_OFF_IN_PERCENT = 0.02\n",
        "\n",
        "# Clustering data set\n",
        "# http://cs.joensuu.fi/sipu/datasets/\n",
        "def download_data_set(file_url = 'http://cs.joensuu.fi/sipu/datasets/s1.txt'):\n",
        "  local_filename, headers = urllib.request.urlretrieve(url = file_url)\n",
        "  data = np.loadtxt(local_filename)\n",
        "  dims = data.shape[1]\n",
        "  if dims > 2:\n",
        "    return np.delete(data, 2, 1)\n",
        "  return data\n",
        "\n",
        "def gmm_selection(data_set):\n",
        "  lowest_bic = np.infty\n",
        "  bic = []\n",
        "  n_components_range = range(1, min(10, data_set.shape[0]))\n",
        "  cv_types = ['spherical', 'tied', 'diag', 'full']\n",
        "  for cv_type in cv_types:\n",
        "    for n_components in n_components_range:\n",
        "        # Fit a Gaussian mixture with EM\n",
        "        gmm = mixture.GaussianMixture(n_components=n_components, covariance_type=cv_type)\n",
        "        gmm.fit(data_set)\n",
        "        bic.append(gmm.bic(data_set))\n",
        "        if bic[-1] < lowest_bic:\n",
        "            lowest_bic = bic[-1]\n",
        "            best_gmm = gmm\n",
        "  return best_gmm\n",
        "\n",
        "# Get all euclid distance between points with keeping their respective order\n",
        "def get_all_distances(data_set):\n",
        "  arr_leng = data_set.shape[0]\n",
        "  all_distances = []\n",
        "  for i in range(arr_leng):\n",
        "    current_point_distances = []\n",
        "    for j in range(arr_leng):\n",
        "      if i == j:\n",
        "        continue\n",
        "      current_point_distances.append(distance.euclidean(data_set[i], data_set[j]))\n",
        "    all_distances.append(current_point_distances)\n",
        "  return np.array(all_distances)\n",
        "\n",
        "def get_cut_off_distance(distances_data):\n",
        "  arr_leng, dims = distances_data.shape\n",
        "  # Get number of neighbors\n",
        "  number_of_items = math.ceil(CUT_OFF_IN_PERCENT * dims)\n",
        "  all_distances = []\n",
        "  for i in range(arr_leng):\n",
        "    point_distance = np.array(distances_data[i])\n",
        "    # Get number_of_items smallest distance\n",
        "    point_distance = np.partition(point_distance,number_of_items)[:number_of_items]\n",
        "    for dist in point_distance:\n",
        "      all_distances.append(dist)\n",
        "  return statistics.mean(all_distances) \n",
        "\n",
        "def get_density_of_point(point_distance, cut_off_distance):\n",
        "  arr_leng = len(point_distance)\n",
        "  result = 0\n",
        "  for i in range(arr_leng):\n",
        "    if point_distance[i] < cut_off_distance:\n",
        "      result += 1\n",
        "  return result\n",
        "\n",
        "def get_densities(distances_data, cut_off_distance):\n",
        "  arr_leng = distances_data.shape[0]\n",
        "  result = []\n",
        "  for i in range(arr_leng):\n",
        "    result.append(get_density_of_point(distances_data[i], cut_off_distance))\n",
        "  return np.array(result)\n",
        "\n",
        "def get_distribution(data_set):\n",
        "  unique, counts = np.unique(data_set, return_counts=True)\n",
        "  arr_leng = unique.shape[0]\n",
        "  result = []\n",
        "  for i in range(arr_leng):\n",
        "    result.append([unique[i], counts[i]])\n",
        "  return np.array(result)\n",
        "\n",
        "def density_groups(densities, densities_distribution, groups):\n",
        "  result = []\n",
        "  for i in range(len(densities)):\n",
        "    current_density = densities[i]\n",
        "    for j in range(len(densities_distribution)):\n",
        "      if current_density == densities_distribution[j][0]:\n",
        "        result.append(groups[j])\n",
        "        break\n",
        "  return np.array(result)\n",
        "  "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgRv9qL0vGxU",
        "colab_type": "text"
      },
      "source": [
        "Download data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxJuyksbvIQ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download data set\n",
        "raw_data_set = download_data_set('http://cs.joensuu.fi/sipu/datasets/R15.txt')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-MxoK4GUGOD",
        "colab_type": "text"
      },
      "source": [
        "Download data, calculate all relevant variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld46kkgKUGeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate all distances\n",
        "all_distances = get_all_distances(raw_data_set)\n",
        "\n",
        "# get cut off distance\n",
        "cut_off_distance = get_cut_off_distance(all_distances)\n",
        "\n",
        "# calculate densitive base on cut off distance\n",
        "densities = get_densities(all_distances, cut_off_distance)\n",
        "\n",
        "# form the distribution to perform gmm selection\n",
        "densities_distribution = get_distribution(densities)\n",
        "\n",
        "# get gmm models\n",
        "gmm_model = gmm_selection(densities_distribution)\n",
        "\n",
        "# get respective groups\n",
        "groups = gmm_model.predict(densities_distribution)\n",
        "\n",
        "# get a respective vector with original densities to represent the desitive of respective point\n",
        "desities_groups = density_groups(densities, densities_distribution, groups)\n",
        "\n",
        "# Referenced of the original array for futher classification point\n",
        "densities_argsort = densities.argsort()[::-1]\n",
        "\n",
        "# Copy to avoid sam referenced then sort in decending order\n",
        "sorted_desities = np.array(densities)\n",
        "sorted_desities.sort()\n",
        "sorted_desities = sorted_desities[::-1]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svNkPkZS6hxG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8a720f9f-ccd1-472b-f728-aa969ccb69a7"
      },
      "source": [
        "abc = np.array([1,2,3,4,5,6,8,7])\n",
        "xxx = abc.argsort()[::-1]\n",
        "print(abc)\n",
        "abc.sort()\n",
        "abc = abc[::-1]\n",
        "print(abc)\n",
        "print(xxx)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 3 4 5 6 8 7]\n",
            "[8 7 6 5 4 3 2 1]\n",
            "19\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}