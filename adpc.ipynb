{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "adpc.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSj6a1hiTWPC",
        "colab_type": "text"
      },
      "source": [
        "Declaration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAMCm9Y7TaVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import urllib.request\n",
        "from sklearn import mixture\n",
        "from scipy.spatial import distance\n",
        "import math\n",
        "import statistics \n",
        "\n",
        "CUT_OFF_IN_PERCENT = 0.02\n",
        "\n",
        "# Clustering data set\n",
        "# http://cs.joensuu.fi/sipu/datasets/\n",
        "def download_data_set(file_url = 'http://cs.joensuu.fi/sipu/datasets/s1.txt'):\n",
        "  local_filename, headers = urllib.request.urlretrieve(url = file_url)\n",
        "  data = np.loadtxt(local_filename)\n",
        "  dims = data.shape[1]\n",
        "  if dims > 2:\n",
        "    return np.delete(data, 2, 1)\n",
        "  return data\n",
        "\n",
        "def gmm_selection(data_set):\n",
        "  lowest_bic = np.infty\n",
        "  bic = []\n",
        "  n_components_range = range(1, min(10, data_set.shape[0]))\n",
        "  cv_types = ['spherical', 'tied', 'diag', 'full']\n",
        "  for cv_type in cv_types:\n",
        "    for n_components in n_components_range:\n",
        "        # Fit a Gaussian mixture with EM\n",
        "        gmm = mixture.GaussianMixture(n_components=n_components, covariance_type=cv_type)\n",
        "        gmm.fit(data_set)\n",
        "        bic.append(gmm.bic(data_set))\n",
        "        if bic[-1] < lowest_bic:\n",
        "            lowest_bic = bic[-1]\n",
        "            best_gmm = gmm\n",
        "  return best_gmm\n",
        "\n",
        "# Get all euclid distance between points with keeping their respective order\n",
        "def get_all_distances(data_set):\n",
        "  arr_leng = data_set.shape[0]\n",
        "  all_distances = []\n",
        "  for i in range(arr_leng):\n",
        "    current_point_distances = []\n",
        "    for j in range(arr_leng):\n",
        "      if i == j:\n",
        "        continue\n",
        "      current_point_distances.append(distance.euclidean(data_set[i], data_set[j]))\n",
        "    all_distances.append(current_point_distances)\n",
        "  return np.array(all_distances)\n",
        "\n",
        "def get_cut_off_distance(distances_data):\n",
        "  arr_leng, dims = distances_data.shape\n",
        "  # Get number of neighbors\n",
        "  number_of_items = math.ceil(CUT_OFF_IN_PERCENT * dims)\n",
        "  all_distances = []\n",
        "  for i in range(arr_leng):\n",
        "    point_distance = np.array(distances_data[i])\n",
        "    # Get number_of_items smallest distance\n",
        "    point_distance = np.partition(point_distance,number_of_items)[:number_of_items]\n",
        "    for dist in point_distance:\n",
        "      all_distances.append(dist)\n",
        "  return statistics.mean(all_distances) \n",
        "\n",
        "def get_density_of_point(point_distance, cut_off_distance):\n",
        "  arr_leng = len(point_distance)\n",
        "  result = 0\n",
        "  for i in range(arr_leng):\n",
        "    if point_distance[i] < cut_off_distance:\n",
        "      result += 1\n",
        "  return result\n",
        "\n",
        "def get_densities(distances_data, cut_off_distance):\n",
        "  arr_leng = distances_data.shape[0]\n",
        "  result = []\n",
        "  for i in range(arr_leng):\n",
        "    result.append(get_density_of_point(distances_data[i], cut_off_distance))\n",
        "  return np.array(result)\n",
        "\n",
        "def get_distribution(data_set):\n",
        "  unique, counts = np.unique(data_set, return_counts=True)\n",
        "  arr_leng = unique.shape[0]\n",
        "  result = []\n",
        "  for i in range(arr_leng):\n",
        "    result.append([unique[i], counts[i]])\n",
        "  return np.array(result)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-MxoK4GUGOD",
        "colab_type": "text"
      },
      "source": [
        "Download data, calculate all relevant variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld46kkgKUGeW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "r15 = download_data_set('http://cs.joensuu.fi/sipu/datasets/R15.txt')\n",
        "all_distances = get_all_distances(r15)\n",
        "cut_off_distance = get_cut_off_distance(all_distances)\n",
        "densities = get_densities(all_distances, cut_off_distance * 0.5)\n",
        "densities_distribution = get_distribution(densities)\n",
        "gmm_model = gmm_selection(densities_distribution)\n",
        "groups = gmm_model.predict(densities_distribution)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svNkPkZS6hxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(groups)\n",
        "print(densities_distribution)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}